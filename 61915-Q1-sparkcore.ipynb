{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVUioJpsqZS-"
      },
      "source": [
        "### Question 1 - Daily and monthly statistics\n",
        "\n",
        "  For each sensor, compute the minimum, average and maximum values of the two sensor metrics. Produce results for each day.\n",
        "\n",
        " **Requirement**: Solve this question using MapReduce (MrJob) and Spark Core."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeR7fAF6BTN9",
        "outputId": "be82cea8-0f8f-4f6c-e9c4-7e5d00b0ace7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#@title Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # Faz o mount da drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WJoYvhszjCNL"
      },
      "outputs": [],
      "source": [
        "#@title Install MRJob and load dataset\n",
        "!pip install mrjob --quiet # Faz a instalação do MRJob\n",
        "!wget -q -O /etc/mrjob.conf https://raw.githubusercontent.com/smduarte/spbd-2223/main/lab2/mrjob.conf # Faz a configuração do MRJob\n",
        "# Faz o import dos dados\n",
        "!head -10000 /content/drive/MyDrive/projeto_spbd/sds011-2020-01-01.csv > files\n",
        "!head -10000 /content/drive/MyDrive/projeto_spbd/sds011-2020-01-02.csv >> files\n",
        "!head -10000 /content/drive/MyDrive/projeto_spbd/sds011-2020-01-03.csv >> files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "AP0EWG2chyZx"
      },
      "outputs": [],
      "source": [
        "#@title Install Pyspark\n",
        "!pip install --quiet pyspark # Faz a instalação do Pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXHOzWvblBZA",
        "outputId": "0893cad1-69c4-4627-9f34-510828e914dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(('1000', '2020-01-01'), (341.53, 297.97, 319.75, 275.53, 239.2, 257.365))\n",
            "(('1000', '2020-01-02'), (12.13, 10.53, 11.186666666666667, 9.0, 7.37, 8.223333333333334))\n",
            "(('1000', '2020-01-03'), (18.0, 15.0, 16.5, 13.73, 12.4, 13.065000000000001))\n",
            "(('10009', '2020-01-01'), (75.3, 69.03, 72.16499999999999, 44.77, 44.53, 44.650000000000006))\n",
            "(('10009', '2020-01-02'), (21.53, 21.0, 21.265, 12.33, 12.3, 12.315000000000001))\n",
            "(('10009', '2020-01-03'), (14.17, 12.23, 13.2, 8.4, 7.4, 7.9))\n",
            "(('10011', '2020-01-01'), (112.47, 105.67, 109.07, 56.9, 52.9, 54.9))\n",
            "(('10011', '2020-01-02'), (35.6, 35.37, 35.485, 16.77, 16.67, 16.72))\n",
            "(('10011', '2020-01-03'), (24.37, 23.23, 23.8, 12.1, 11.33, 11.715))\n",
            "(('10029', '2020-01-01'), (5.57, 5.57, 5.57, 5.1, 5.1, 5.1))\n",
            "(('10029', '2020-01-02'), (0.98, 0.6, 0.79, 0.98, 0.6, 0.79))\n",
            "(('10029', '2020-01-03'), (0.7, 0.43, 0.565, 0.7, 0.43, 0.565))\n",
            "(('1004', '2020-01-01'), (637.47, 478.33, 557.9, 455.2, 331.97, 393.58500000000004))\n",
            "(('1004', '2020-01-02'), (57.1, 56.43, 56.765, 29.93, 28.97, 29.45))\n",
            "(('1004', '2020-01-03'), (23.8, 23.8, 23.8, 15.2, 15.2, 15.2))\n",
            "(('10041', '2020-01-01'), (126.47, 126.47, 126.47, 57.27, 57.27, 57.27))\n",
            "(('10041', '2020-01-02'), (70.6, 66.8, 68.69999999999999, 31.1, 30.4, 30.75))\n",
            "(('10041', '2020-01-03'), (63.83, 63.83, 63.83, 19.1, 19.1, 19.1))\n",
            "(('10056', '2020-01-01'), (157.0, 119.73, 137.91, 102.93, 76.5, 89.84333333333332))\n",
            "(('10056', '2020-01-02'), (12.03, 9.47, 10.423333333333334, 7.07, 6.93, 7.010000000000001))\n"
          ]
        }
      ],
      "source": [
        "#@title Resolution using Spark Core\n",
        "import pyspark\n",
        "from operator import *\n",
        "\n",
        "sc = pyspark.SparkContext('local[*]') # Cria o SparkContext, em local mode\n",
        "\n",
        "try:\n",
        "  lines = sc.textFile('/content/files').map( lambda line: line.strip() ) # Faz o carregamento dos dados e o strip de cada linha do ficheiro\n",
        "  \n",
        "  sensors = lines.map( lambda line: line.split(';')) # Faz o split de cada linha do ficheiro csv, dividindo-o em 6 campos\n",
        "  sensors1 = sensors.filter( lambda values: len(values) > 0) # Faz o filtro para reter, apenas, as linhas que são superiores a zero, ou seja, não nulas\n",
        "  sensors2 = sensors1.map( lambda values: ((values[0],values[4][0:10]), (float(values[5]), float(values[6])))) # Faz o map, sendo o número do sensor a key e a tuple (p1, p2), em float, a value\n",
        "  sensors3 = sensors2.map( lambda kv : (kv[0], (1, kv[1][0], kv[1][0], kv[1][0], kv[1][1], kv[1][1], kv[1][1]))) # Faz o map, sendo o número de do sensor a key e a tuple (1, p1, p1, p1, p2, p2, p2)\n",
        "  sensors4 = sensors3.reduceByKey( lambda a, b : (a[0] + b[0], a[1] + b[1], max(a[2],b[2]), min(a[3],b[3]), a[4] + b[4], max(a[5],b[5]), min(a[6],b[6])) ) # Faz o reduceByKey, criando uma value correspondente a: (1+1, p1+p1, max(p1,p1), min(p1,p1), p2+p2, max(p2,p2), min(p2,p2)). Soma 1+1, p1+p1 e p2+p2, de cada linha, e preserva o min e max para cada valor de p1 e p2. \n",
        "  sensors5 = sensors4.map( lambda kv : ( kv[0] , (kv[1][2] , kv[1][3] , kv[1][1] / kv[1][0] , kv[1][5] , kv[1][6], kv[1][4] / kv[1][0]))) # Faz o map, sendo o número do sensor a key e a tuple (max p1, min p1, mean p1, max p2, min p2, mean p2)\n",
        "  sensors6 = sensors5.sortByKey() # Faz o sortByKey, por ordem crescente do número do sensor\n",
        "       \n",
        "  for sensor in sensors6.take(20): \n",
        "    print(sensor)\n",
        "  # Faz o print dos 20 primeiros valores\n",
        "  sc.stop()\n",
        "except Exception as e: \n",
        "  print(e)\n",
        "  sc.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
